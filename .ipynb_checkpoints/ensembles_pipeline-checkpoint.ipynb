{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from ipynb.fs.full.data_wrangling import * #Data preprocessing notebook\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "\n",
    "\n",
    "SCORINGS = {\n",
    "    'f1': make_scorer(f1_score, average = None),\n",
    "    'precision': make_scorer(precision_score, average = None),\n",
    "    'recall': make_scorer(recall_score, average = None),\n",
    "    'roc_auc': make_scorer(roc_auc_score, average = None)\n",
    "}\n",
    "\n",
    "cv_splits = 10\n",
    "repetitions = 1\n",
    "RANDOM_STATE = 42\n",
    "cross_validation_setting = RepeatedStratifiedKFold(n_splits=cv_splits,\n",
    "                                                   n_repeats=repetitions,\n",
    "                                                   random_state= RANDOM_STATE)\n",
    "        \n",
    "def cross_validate(model, X_train, X_test, y_train, y_test, metric):\n",
    "    for index in range(len(X_train)):\n",
    "        model.fit(X_train, y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "def model_evaluation(model, features, target, \n",
    "                     cv = cross_validation_setting):\n",
    "    scores = dict()\n",
    "    formatted_scores = dict()\n",
    "    formatted_scores['model'] = model\n",
    "    for scoring_name, scoring_function in SCORINGS.items():\n",
    "        scores[score_metric] = cross_validate(model, X, Y, \n",
    "                                              scoring = scoring_function,\n",
    "                                              cv = cross_validation_setting)\n",
    "        return scores[score_metric]\n",
    "#         formatted_scores[score_metric + \" score\"] = scores[score_metric].mean()\n",
    "#         formatted_scores[score_metric + \" std\"] = scores[score_metric].std()\n",
    "#     return formatted_scores\n",
    "\n",
    "def get_scores(Y_pred, Y_true):\n",
    "    f1 = f1_score(Y_true, Y_pred, average=None)\n",
    "    precision = precision_score(Y_true, Y_pred,\n",
    "                                average = None)\n",
    "    recall = recall_score(Y_true, Y_pred, \n",
    "                          average = None)\n",
    "    roc = roc_auc_score(Y_true, Y_pred, \n",
    "                        average = None)\n",
    "    return f1, precision, recall, roc\n",
    "\n",
    "def convert_df(X, Y):\n",
    "    return X.to_numpy(), Y.to_numpy()\n",
    "    \n",
    "def evaluate_model(model, X, Y, sk_fold):\n",
    "    X, Y = convert_df(X, Y)\n",
    "    f1_list, precision_list, recall_list, auc_list = [], [], [], []\n",
    "    \n",
    "    for train_index, test_index in sk_fold.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "    \n",
    "        f1, precision, recall, roc = get_scores(Y_pred, Y_test)\n",
    "        f1_list.append(f1)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        auc_list.append(roc)\n",
    "        \n",
    "    return format_return(model, f1_list, precision_list,\n",
    "                         recall_list, auc_list)\n",
    "\n",
    "COLUMNS = [\"Model Name\", \"F1 Average\", \"F1 Class 0\", \"F1 Class 1\",\n",
    "           \"Recall Average\", \"Recall Class 0\", \"Recall Class 1\",\n",
    "#            \"AUC Average\", \"AUC Class 0\", \"AUC Class 1\",\n",
    "           \"Precision Average\", \"Precision Class 0\", \"Precision Class 1\"]\n",
    "\n",
    "def format_return(model_name, f1_list, precision_list,\n",
    "                  recall_list, auc_list):\n",
    "    dataframe = pd.DataFrame(columns = COLUMNS)\n",
    "    dataframe_line = []\n",
    "    dataframe_line.append(model_name)\n",
    "    aux = []\n",
    "    lists_of_score_list = []\n",
    "    lists_of_score_list.append(f1_list)\n",
    "    lists_of_score_list.append(precision_list)\n",
    "    lists_of_score_list.append(recall_list)\n",
    "    # auc_list\n",
    "    for score_list in lists_of_score_list:\n",
    "        dataframe_line.extend((statistics.mean(flatten_list(score_list)),\n",
    "                               statistics.mean([score[0] for score in score_list]),\n",
    "                               statistics.mean([score[1] for score in score_list])))\n",
    "    \n",
    "    return dataframe.append(pd.Series(dataframe_line, index = COLUMNS),\n",
    "                            ignore_index = True)\n",
    "\n",
    "def flatten_list(lista):\n",
    "    return [value for sublist in lista for value in sublist]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Config for every experiment\n",
    "\"\"\"\n",
    "RANDOM_STATE = 0\n",
    "VOTING_METHOD = 'soft'\n",
    "results = pd.DataFrame(columns = COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting the best weak learners (GridSearchCV section)\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Average</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "      <th>Recall Average</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>Precision Average</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=6, weights='d...</td>\n",
       "      <td>0.611198</td>\n",
       "      <td>0.578264</td>\n",
       "      <td>0.644132</td>\n",
       "      <td>0.615988</td>\n",
       "      <td>0.617013</td>\n",
       "      <td>0.614964</td>\n",
       "      <td>0.612646</td>\n",
       "      <td>0.546831</td>\n",
       "      <td>0.678462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model Name  F1 Average  F1 Class 0  \\\n",
       "0  KNeighborsClassifier(n_neighbors=6, weights='d...    0.611198    0.578264   \n",
       "\n",
       "   F1 Class 1  Recall Average  Recall Class 0  Recall Class 1  \\\n",
       "0    0.644132        0.615988        0.617013        0.614964   \n",
       "\n",
       "   Precision Average  Precision Class 0  Precision Class 1  \n",
       "0           0.612646           0.546831           0.678462  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "K Neighbors\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_range = list(range(1,31))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "\n",
    "knn_grid = dict(n_neighbors = k_range, \n",
    "                weights = weight_options)\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gridsearch = GridSearchCV(knn, param_grid = knn_grid,\n",
    "                             refit = True)\n",
    "selected_knn = knn_gridsearch.fit(X, Y).best_estimator_\n",
    "\n",
    "results.append(evaluate_model(selected_knn, X, Y, cross_validation_setting))\n",
    "#results = results.append(model_evaluation(selected_knn, X , Y), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Average</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "      <th>Recall Average</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>Precision Average</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model Name, F1 Average, F1 Class 0, F1 Class 1, Recall Average, Recall Class 0, Recall Class 1, Precision Average, Precision Class 0, Precision Class 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_grid={'C':[0.001,0.01,.09,1,5,10],\n",
    "              \"penalty\":[\"l1\",\"l2\"]} #l1 lasso l2 ridge\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "log_reg_gridsearch = GridSearchCV(lr, param_grid = log_reg_grid,\n",
    "                             refit = True)\n",
    "selected_log_reg = log_reg_gridsearch.fit(X,Y).best_estimator_\n",
    "evaluate_model(selected_log_reg, X.to_numpy(), Y.to_numpy())\n",
    "#results = results.append(model_evaluation(selected_log_reg, X, Y), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Support Vector Machines\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']} \n",
    "  \n",
    "svm_gridsearch = GridSearchCV(SVC(),\n",
    "                              param_grid = svm_grid,\n",
    "                              refit = True)\n",
    "\n",
    "selected_svm = svm_gridsearch.fit(X,Y).best_estimator_\n",
    "model_evaluation(selected_svm, X , Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "nb_gridsearch = GridSearchCV(estimator=nb, \n",
    "                     param_grid = nb_params,\n",
    "                     refit = True)\n",
    "selected_nb = nb_gridsearch.fit(X,Y).best_estimator_\n",
    "model_evaluation(selected_nb, X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Boosting Algorithms\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boosting_models = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost\n",
    "ab_param_grid = {\n",
    "    'n_estimators' : [100, 300, 500],\n",
    "    'learning_rate' : [1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "ab_model = AdaBoostClassifier(random_state = RANDOM_STATE)\n",
    "xgb_gridsearch = GridSearchCV(ab_model,\n",
    "                              param_grid = ab_param_grid,\n",
    "                              refit = True)\n",
    "\n",
    "selected_xgb = xgb_gridsearch.fit(X, Y).best_estimator_\n",
    "model_evaluation(selected_xgb, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "parameters = {\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'n_estimators': [200, 400],\n",
    "            'gamma': [0.01, 0.1, 0.2],\n",
    "            'min_child_weight': [0, 0.5, 1],\n",
    "            'max_delta_step': [0],\n",
    "            'subsample': [0.7, 1],\n",
    "            'colsample_bytree': [0.6, 1],\n",
    "            'reg_alpha': [0, 1e-2, 1],\n",
    "            'reg_lambda': [0, 1e-2, 1],\n",
    "            }\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(silent = True,\n",
    "                              random_state = RANDOM_STATE)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(xgb_model,\n",
    "                              parameters,\n",
    "                              refit = True)\n",
    "\n",
    "selected_xgb = xgb_gridsearch.fit(X, Y).best_estimator_\n",
    "model_evaluation(selected_xgb, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bagging Algorithms\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_models = dict()\n",
    "bagging_models['rf'] = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "bagging_models['et'] = ExtraTreesClassifier(random_state = RANDOM_STATE)\n",
    "bagging_models['bdt'] = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                          random_state = RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "rf_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [200, 400]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "rf_gridsearch = GridSearchCV(rf, param_grid = rf_grid,\n",
    "                             refit = True)\n",
    "selected_rf = rf_gridsearch.fit(X, Y).best_estimator_\n",
    "model_evaluation(selected_rf, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Trees\n",
    "\n",
    "et_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [200, 400],\n",
    "    'oob_score': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stacking Ensemble\"\"\"\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('lr', selected_lr),\n",
    "    ('knn', selected_knn),\n",
    "    ('svm', selected_svm),\n",
    "    ('gnb', selected_nb),\n",
    "    ('dt', selected_dt)\n",
    "]\n",
    "\n",
    "final_estimator = selected_lr\n",
    "\n",
    "stacking_model = StackingClassifier(estimators = stacking_estimators,\n",
    "                                    final_estimator = final_estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Voting Ensemble\"\"\"\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_estimators = [\n",
    "    ('lr', LogisticRegression(random_state=RANDOM_STATE)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('svm', SVC(random_state=RANDOM_STATE)),\n",
    "    ('gnb', GaussianNB()),\n",
    "    ('dt', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(voting_estimators,\n",
    "                                     voting=VOTING_METHOD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
