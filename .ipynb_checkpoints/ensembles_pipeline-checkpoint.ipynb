{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\nathan\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\nathan\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.19.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\nathan\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\nathan\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\nathan\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nathan\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "from imblearn.datasets import fetch_datasets\n",
    "X, Y = [], []\n",
    "dataset_names = ['us_crime', 'car_eval_34']\n",
    "# dataset_names = ['ecoli', 'optical_digits']\n",
    "# , 'satimage', 'pen_digits',\n",
    "#                  'abalone', 'sick_euthyroid', 'spectrometer',\n",
    "#                  'car_eval_34', 'isolet', 'us_crime', 'yeast_ml8',\n",
    "#                  'scene']\n",
    "for ds_name in dataset_names:\n",
    "    var = fetch_datasets()[ds_name]\n",
    "    X.append(var['data'])\n",
    "    Y.append(var['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from ipynb.fs.full.data_wrangling import * #Data preprocessing notebook\n",
    "#from ipynb.fs.full.data_preparation import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SCORINGS = {\n",
    "    'f1': make_scorer(f1_score, average = None),\n",
    "    'precision': make_scorer(precision_score, average = None),\n",
    "    'recall': make_scorer(recall_score, average = None),\n",
    "    'roc_auc': make_scorer(roc_auc_score, average = None)\n",
    "}\n",
    "\n",
    "cv_splits = 10\n",
    "repetitions = 1\n",
    "RANDOM_STATE = 42\n",
    "cross_validation_setting = RepeatedStratifiedKFold(n_splits=cv_splits,\n",
    "                                                   n_repeats=repetitions,\n",
    "                                                   random_state= RANDOM_STATE)\n",
    "        \n",
    "def cross_validate(model, X_train, X_test, y_train, y_test, metric):\n",
    "    for index in range(len(X_train)):\n",
    "        model.fit(X_train, y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "def model_evaluation(model, features, target, \n",
    "                     cv = cross_validation_setting):\n",
    "    scores = dict()\n",
    "    formatted_scores = dict()\n",
    "    formatted_scores['model'] = model\n",
    "    for scoring_name, scoring_function in SCORINGS.items():\n",
    "        scores[score_metric] = cross_validate(model, X, Y, \n",
    "                                              scoring = scoring_function,\n",
    "                                              cv = cross_validation_setting)\n",
    "        return scores[score_metric]\n",
    "#         formatted_scores[score_metric + \" score\"] = scores[score_metric].mean()\n",
    "#         formatted_scores[score_metric + \" std\"] = scores[score_metric].std()\n",
    "#     return formatted_scores\n",
    "\n",
    "def get_scores(Y_pred, Y_true):\n",
    "    f1 = f1_score(Y_true, Y_pred, average=None)\n",
    "    precision = precision_score(Y_true, Y_pred,\n",
    "                                average = None)\n",
    "    recall = recall_score(Y_true, Y_pred, \n",
    "                          average = None)\n",
    "    roc = roc_auc_score(Y_true, Y_pred, \n",
    "                        average = None)\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    \n",
    "    return f1, precision, recall, roc, acc\n",
    "\n",
    "def convert_df(X, Y):\n",
    "    return X.to_numpy(), Y.to_numpy()\n",
    "\n",
    "def fault_cases(predictions, answers, indexes):\n",
    "    failed_cases = []\n",
    "    for i in range(len(predictions)):\n",
    "        if (predictions[i] != answers[i]):\n",
    "            failed_cases.append(indexes[i])\n",
    "    return failed_cases\n",
    "\n",
    "def get_best_estimator(model, param_grid, X, Y):\n",
    "    model_gridsearch = GridSearchCV(model, param_grid#,\n",
    "                                    #refit = True\n",
    "                                   )\n",
    "    return model_gridsearch.fit(X, Y).best_estimator_\n",
    "    \n",
    "def evaluate_model(dataset_name, models,\n",
    "                   hyperparameters_grid, X, Y, sk_fold,\n",
    "                   metric = \"F1 Average B\"):\n",
    "    \n",
    "#     X, Y = convert_df(X, Y)\n",
    "\n",
    "    best_metric_score = 0\n",
    "    best_failed_cases = []\n",
    "    folds = sk_fold.split(X, Y)\n",
    "    model_names = list(weak_learners_base_models.keys())\n",
    "    for model_name in model_names:\n",
    "        print (\"Model Name: \" + model_name)\n",
    "        estimator = get_best_estimator(models[model_name],\n",
    "                                       hyperparameters_grid[model_name],\n",
    "                                       X, Y)\n",
    "        print (\"Estimator: \")\n",
    "        print(estimator)\n",
    "        f1_list, precision_list, recall_list, auc_list, acc_list = [], [], [], [], []\n",
    "        aux_failed_cases_index = []\n",
    "        failed_cases_index = []\n",
    "        for train_index, test_index in sk_fold.split(X, Y)\n",
    ":\n",
    "            print (\"Model Name: \" + model_name)\n",
    "            print (\"Passou\")\n",
    "\n",
    "            model_estimator = estimator\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            model_estimator.fit(X_train, Y_train)\n",
    "            \n",
    "            Y_pred = model_estimator.predict(X_test)\n",
    "            f1, precision, recall, roc, acc = get_scores(Y_pred, Y_test)\n",
    "\n",
    "            failed_cases_index.append(fault_cases(Y_pred, Y_test, test_index))\n",
    "            f1_list.append(f1)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            auc_list.append(roc)\n",
    "            acc_list.append(acc)\n",
    "            \n",
    "        print (\"F1: \")\n",
    "        print (f1_list)\n",
    "        print (\"Prec: \")\n",
    "        print (precision_list)\n",
    "        print (\"Rec: \")\n",
    "        print (recall_list)\n",
    "        print(\"Auc: \")\n",
    "        print (auc_list)\n",
    "        print (\"Acc: \")\n",
    "        print (acc_list)\n",
    "            \n",
    "        aux_table, aux_failed_cases_index =  format_return(dataset_name, model_name,\n",
    "                                                           f1_list, precision_list,\n",
    "                                                           recall_list, auc_list,\n",
    "                                                           acc_list), failed_cases_index[0]\n",
    "        print(\"Model: \")\n",
    "        print (aux_table)\n",
    "        \n",
    "        if aux_table.iloc[0][metric] > best_metric_score:\n",
    "            best_model_table = aux_table\n",
    "            best_metric_score = aux_table.iloc[0][metric]\n",
    "            best_failed_cases = aux_failed_cases_index\n",
    "            \n",
    "    return best_model_table, best_failed_cases\n",
    "\n",
    "COLUMNS = [\"Dataset Name\", \"Base Model Name\", \"F1 Average B\", \"F1 Class 0 B\", \"F1 Class 1 B\",\n",
    "           \"Recall Average B\", \"Recall Class 0 B\", \"Recall Class 1 B\",\n",
    "           \"Precision Average B\", \"Precision Class 0 B\", \"Precision Class 1 B\",\n",
    "           \"AUC B\", \"Accuracy B\"]\n",
    "\n",
    "ENSEMBLE_COLUMNS = [\"Ensemble Model Name\", \"F1 Average E\", \"F1 Class 0 E\", \"F1 Class 1 E\",\n",
    "                   \"Recall Average E\", \"Recall Class 0 E\", \"Recall Class 1 E\",\n",
    "                   \"Precision Average E\", \"Precision Class 0 E\", \"Precision Class 1 E\",\n",
    "                   \"AUC E\", \"Accuracy E\"]\n",
    "\n",
    "def format_return(dataset_name, model_name, f1_list, precision_list,\n",
    "                  recall_list, auc_list, acc_list):\n",
    "    dataframe = pd.DataFrame(columns = COLUMNS)\n",
    "    dataframe_line = []\n",
    "    dataframe_line.append(dataset_name)\n",
    "    dataframe_line.append(model_name)\n",
    "    aux = []\n",
    "    lists_of_score_list = []\n",
    "    lists_of_score_list.append(f1_list)\n",
    "    lists_of_score_list.append(precision_list)\n",
    "    lists_of_score_list.append(recall_list)\n",
    "    # auc_list\n",
    "    print (lists_of_score_list)\n",
    "    for score_list in lists_of_score_list:\n",
    "        dataframe_line.extend((statistics.mean(flatten_list(score_list)),\n",
    "                               statistics.mean([score[0] for score in score_list]),\n",
    "                               statistics.mean([score[1] for score in score_list])))\n",
    "        print(\"Size: \")\n",
    "        print(len(dataframe_line))\n",
    "    dataframe_line.append(statistics.mean(auc_list))\n",
    "    dataframe_line.append(statistics.mean(acc_list))\n",
    "    return dataframe.append(pd.Series(dataframe_line, index = COLUMNS),\n",
    "                            ignore_index = True)\n",
    "\n",
    "def flatten_list(lista):\n",
    "    return [value for sublist in lista for value in sublist]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Config for every experiment\n",
    "\"\"\"\n",
    "RANDOM_STATE = 0\n",
    "VOTING_METHOD = 'hard'\n",
    "results = pd.DataFrame(columns = COLUMNS)\n",
    "\n",
    "param_grids = dict()\n",
    "weak_learners_base_models = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Weak Learners\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "KNN\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k_range = list(range(1,31))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "\n",
    "knn_grid = dict(n_neighbors = k_range, \n",
    "                weights = weight_options)\n",
    "\n",
    "param_grids[\"K-Nearest Neighbors\"] = knn_grid\n",
    "knn = KNeighborsClassifier()\n",
    "weak_learners_base_models[\"K-Nearest Neighbors\"] = knn\n",
    "\n",
    "# knn_gridsearch = GridSearchCV(knn, param_grid = knn_grid,\n",
    "#                              refit = True)\n",
    "# selected_knn = knn_gridsearch.fit(X, Y).best_estimator_\n",
    "# failed_cases = []\n",
    "# scores, failed = evaluate_model(\"KNN\", selected_knn, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n",
    "# results = results.append(model_evaluation(selected_knn, X , Y), ignore_index = True)\n",
    "\n",
    "\"\"\"\n",
    "LR\n",
    "\"\"\"\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# log_reg_grid={'C':[0.001,0.01,.09,1,5,10],\n",
    "#               \"penalty\":[\"l1\",\"l2\"]} #l1 lasso l2 ridge\n",
    "# lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "# param_grids[\"Logistic Regression\"] = log_reg_grid\n",
    "# weak_learners_base_models[\"Logistic Regression\"] = lr\n",
    "\n",
    "\"\"\"\n",
    "Support Vector Machines\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']}\n",
    "\n",
    "param_grids[\"Support Vector Machines\"] = svm_grid\n",
    "weak_learners_base_models[\"Support Vector Machines\"] = SVC()\n",
    "\"\"\"\n",
    "Naive Bayes\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "weak_learners_base_models[\"Naive Bayes\"] = nb\n",
    "param_grids[\"Naive Bayes\"] = nb_params\n",
    "\n",
    "\"\"\"\n",
    "DECISION TREES\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_parameters = {'criterion':['gini','entropy'],\n",
    "             'max_depth':[4,5,10]}\n",
    "\n",
    "param_grids['Decision Tree'] = tree_parameters\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "weak_learners_base_models['Decision Tree'] = dt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: K-Nearest Neighbors\n",
      "Estimator: \n",
      "KNeighborsClassifier(n_neighbors=8)\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "Model Name: K-Nearest Neighbors\n",
      "Passou\n",
      "F1: \n",
      "[array([0.96858639, 0.33333333]), array([0.97112861, 0.42105263]), array([0.96335079, 0.22222222]), array([0.97340426, 0.58333333]), array([0.96      , 0.34782609]), array([0.97082228, 0.47619048]), array([0.97354497, 0.5       ]), array([0.96062992, 0.11764706]), array([0.96      , 0.34782609]), array([0.95514512, 0.10526316])]\n",
      "Prec: \n",
      "[array([0.93908629, 1.        ]), array([0.94387755, 1.        ]), array([0.93401015, 0.66666667]), array([0.95811518, 0.77777778]), array([0.94240838, 0.5       ]), array([0.94818653, 0.83333333]), array([0.94845361, 1.        ]), array([0.92893401, 0.5       ]), array([0.94240838, 0.5       ]), array([0.92820513, 0.25      ])]\n",
      "Rec: \n",
      "[array([1. , 0.2]), array([1.        , 0.26666667]), array([0.99459459, 0.13333333]), array([0.98918919, 0.46666667]), array([0.97826087, 0.26666667]), array([0.99456522, 0.33333333]), array([1.        , 0.33333333]), array([0.99456522, 0.06666667]), array([0.97826087, 0.26666667]), array([0.98369565, 0.06666667])]\n",
      "Auc: \n",
      "[0.6, 0.6333333333333333, 0.563963963963964, 0.7279279279279279, 0.622463768115942, 0.6639492753623187, 0.6666666666666666, 0.5306159420289854, 0.622463768115942, 0.5251811594202899]\n",
      "Acc: \n",
      "[0.94, 0.945, 0.93, 0.95, 0.9246231155778895, 0.9447236180904522, 0.949748743718593, 0.9246231155778895, 0.9246231155778895, 0.914572864321608]\n",
      "[[array([0.96858639, 0.33333333]), array([0.97112861, 0.42105263]), array([0.96335079, 0.22222222]), array([0.97340426, 0.58333333]), array([0.96      , 0.34782609]), array([0.97082228, 0.47619048]), array([0.97354497, 0.5       ]), array([0.96062992, 0.11764706]), array([0.96      , 0.34782609]), array([0.95514512, 0.10526316])], [array([0.93908629, 1.        ]), array([0.94387755, 1.        ]), array([0.93401015, 0.66666667]), array([0.95811518, 0.77777778]), array([0.94240838, 0.5       ]), array([0.94818653, 0.83333333]), array([0.94845361, 1.        ]), array([0.92893401, 0.5       ]), array([0.94240838, 0.5       ]), array([0.92820513, 0.25      ])], [array([1. , 0.2]), array([1.        , 0.26666667]), array([0.99459459, 0.13333333]), array([0.98918919, 0.46666667]), array([0.97826087, 0.26666667]), array([0.99456522, 0.33333333]), array([1.        , 0.33333333]), array([0.99456522, 0.06666667]), array([0.97826087, 0.26666667]), array([0.98369565, 0.06666667])]]\n",
      "Size: \n",
      "5\n",
      "Size: \n",
      "8\n",
      "Size: \n",
      "11\n",
      "Model: \n",
      "  Dataset Name      Base Model Name  F1 Average B  F1 Class 0 B  F1 Class 1 B  \\\n",
      "0     us_crime  K-Nearest Neighbors      0.655565      0.965661      0.345469   \n",
      "\n",
      "   Recall Average B  Recall Class 0 B  Recall Class 1 B  Precision Average B  \\\n",
      "0          0.822073          0.941369          0.702778             0.615657   \n",
      "\n",
      "   Precision Class 0 B  Precision Class 1 B     AUC B  Accuracy B  \n",
      "0             0.991313                 0.24  0.615657    0.934791  \n",
      "Model Name: Support Vector Machines\n",
      "Estimator: \n",
      "SVC(C=1000, gamma=0.001)\n",
      "F1: \n",
      "[]\n",
      "Prec: \n",
      "[]\n",
      "Rec: \n",
      "[]\n",
      "Auc: \n",
      "[]\n",
      "Acc: \n",
      "[]\n",
      "[[], [], []]\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-8a3c746488d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                    \u001b[0mparam_grids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                    \u001b[0mcross_validation_setting\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                    metric = \"F1 Average B\")\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-149faf29a2d3>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(dataset_name, models, hyperparameters_grid, X, Y, sk_fold, metric)\u001b[0m\n\u001b[0;32m    138\u001b[0m                                                            \u001b[0mf1_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                                                            \u001b[0mrecall_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                                                            acc_list), failed_cases_index[0]\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-149faf29a2d3>\u001b[0m in \u001b[0;36mformat_return\u001b[1;34m(dataset_name, model_name, f1_list, precision_list, recall_list, auc_list, acc_list)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlists_of_score_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mscore_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlists_of_score_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         dataframe_line.extend((statistics.mean(flatten_list(score_list)),\n\u001b[0m\u001b[0;32m    176\u001b[0m                                \u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscore_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                                statistics.mean([score[1] for score in score_list])))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\statistics.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean requires at least one data point'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(columns = COLUMNS)\n",
    "cases = []\n",
    "for index in range(len(dataset_names)):\n",
    "    data = X[index]\n",
    "    target = Y[index]\n",
    "    best, current_case = evaluate_model(dataset_names[index], \n",
    "                   weak_learners_base_models,\n",
    "                   param_grids, data, target,\n",
    "                   cross_validation_setting,\n",
    "                   metric = \"F1 Average B\")\n",
    "    cases.append(current_case)\n",
    "    dataframe = dataframe.append(best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Nathan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "# log_reg_grid={'C':[0.001,0.01,.09,1,5,10],\n",
    "#               \"penalty\":[\"l1\",\"l2\"]} #l1 lasso l2 ridge\n",
    "# lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "# log_reg_gridsearch = GridSearchCV(lr, param_grid = log_reg_grid,\n",
    "#                              refit = True)\n",
    "# selected_log_reg = log_reg_gridsearch.fit(X,Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"Logistic Regression\",selected_log_reg, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Support Vector Machines\n",
    "# \"\"\"\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "# svm_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#             'kernel': ['rbf','poly','sigmoid']} \n",
    "\n",
    "  \n",
    "# svm_gridsearch = GridSearchCV(SVC(),\n",
    "#                               param_grid = svm_grid,\n",
    "#                               refit = True)\n",
    "\n",
    "# selected_svm = svm_gridsearch.fit(X,Y).best_estimator_\n",
    "# selected_svm_stacking = svm_gridsearch2.fit(X,Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"SVM\",selected_svm, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# nb = GaussianNB()\n",
    "\n",
    "# nb_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "# nb_gridsearch = GridSearchCV(estimator=nb, \n",
    "#                      param_grid = nb_params,\n",
    "#                      refit = True)\n",
    "# selected_nb = nb_gridsearch.fit(X,Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"Naive Bayes\",selected_nb, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# tree_para = {'criterion':['gini','entropy'],\n",
    "#              'max_depth':[4,5,10]}\n",
    "# dt = DecisionTreeClassifier(random_state=0)\n",
    "# tree_gridsearch = GridSearchCV(dt, param_grid = tree_para, cv=5)\n",
    "# selected_dt = tree_gridsearch.fit(X, Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"Decision Trees\",selected_dt, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Boosting Algorithms\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boosting_models = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AdaBoost\n",
    "# ab_param_grid = {\n",
    "#     'n_estimators' : [100, 300, 500],\n",
    "#     'learning_rate' : [1e-3, 1e-2, 1e-1, 1]\n",
    "# }\n",
    "# ab_model = AdaBoostClassifier(random_state = RANDOM_STATE)\n",
    "# ab_gridsearchcv = GridSearchCV(ab_model,\n",
    "#                               param_grid = ab_param_grid,\n",
    "#                               refit = True)\n",
    "\n",
    "# selected_ab = ab_gridsearchcv.fit(X, Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"AdaBoost\",selected_ab, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #XGBoost\n",
    "# import xgboost as xgb\n",
    "\n",
    "# parameters = {\n",
    "#             'max_depth': [3, 4, 5],\n",
    "#             'learning_rate': [0.01, 0.1, 1],\n",
    "#             'n_estimators': [200, 400],\n",
    "#             'gamma': [0.01, 0.1, 0.2],\n",
    "#             'min_child_weight': [0, 0.5, 1],\n",
    "#             'max_delta_step': [0],\n",
    "#             'subsample': [0.7, 1],\n",
    "#             'colsample_bytree': [0.6, 1],\n",
    "#             'reg_alpha': [0, 1e-2, 1],\n",
    "#             'reg_lambda': [0, 1e-2, 1],\n",
    "#             }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(silent = True,\n",
    "#                               random_state = RANDOM_STATE)\n",
    "\n",
    "# xgb_gridsearch = GridSearchCV(xgb_model,\n",
    "#                               parameters,\n",
    "#                               refit = True)\n",
    "\n",
    "# selected_xgb = xgb_gridsearch.fit(X, Y).best_estimator_\n",
    "# scores, failed = evaluate_model(\"XGB\", selected_xgb, X, Y, cross_validation_setting)\n",
    "# results = results.append(scores)\n",
    "# failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [200, 400]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "rf_gridsearch = GridSearchCV(rf, param_grid = rf_grid,\n",
    "                             refit = True)\n",
    "selected_rf = rf_gridsearch.fit(X, Y).best_estimator_\n",
    "scores, failed = evaluate_model(\"RandomForest\",selected_rf, X, Y, cross_validation_setting)\n",
    "results = results.append(scores)\n",
    "failed_cases.append(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extra Trees\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# et_grid = {\n",
    "#     'max_depth': [4, 5, 6],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [200, 400],\n",
    "#     'oob_score': [True, False]\n",
    "# }\n",
    "\n",
    "# et_clf = ExtraTreesClassifier(random_state = RANDOM_STATE)\n",
    "# et_gridsearch = GridSearchCV(et_clf, param_grid = et_grid,\n",
    "#                              refit = True)\n",
    "# # selected_et = et_gridsearch.fit(X, Y).best_estimator_\n",
    "# # # scores, failed = evaluate_model(\"ExtraTrees\",selected_et, X, Y, cross_validation_setting)\n",
    "# # # results = results.append(scores)\n",
    "# # # failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BaggingClassifier(DecisionTree) \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_param_grid = {\n",
    "    'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "bg_clf = BaggingClassifier(base_estimator=selected_dt,\n",
    "                           random_state = RANDOM_STATE)\n",
    "bg_gridsearch = GridSearchCV(bg_clf, param_grid = bagging_param_grid,\n",
    "                             refit = True)\n",
    "# selected_bg = bg_gridsearch.fit(X, Y).best_estimator_\n",
    "# # scores, failed = evaluate_model(\"Bagging (Decision Trees)\",selected_bg, X, Y, cross_validation_setting)\n",
    "# # results = results.append(scores)\n",
    "# # failed_cases.append(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stacking Ensemble\"\"\"\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('lr', selected_log_reg),\n",
    "    ('knn', selected_knn),\n",
    "    ('svm', selected_svm_stacking),\n",
    "    ('gnb', selected_nb),\n",
    "    ('dt', selected_dt)\n",
    "]\n",
    "\n",
    "final_estimator = selected_log_reg\n",
    "\n",
    "stacking_model = StackingClassifier(estimators = stacking_estimators,\n",
    "                                    final_estimator = final_estimator)\n",
    "\n",
    "scores, failed = evaluate_model(\"Stacking (LR, KNN, SVM, NB, DT)\",stacking_model, X, Y, cross_validation_setting)\n",
    "results = results.append(scores)\n",
    "failed_cases.append(failed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Voting Ensemble\"\"\"\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_estimators = [\n",
    "    ('lr', selected_log_reg),\n",
    "    ('knn', selected_knn),\n",
    "    ('svm', selected_svm),\n",
    "    ('gnb', selected_nb),\n",
    "    ('dt', selected_dt)\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(voting_estimators,\n",
    "                                     voting=VOTING_METHOD)\n",
    "scores, failed = evaluate_model(\"Voting (LR, KNN, SVM, NB, DT)\",voting_classifier, X, Y, cross_validation_setting)\n",
    "results = results.append(scores)\n",
    "failed_cases.append(failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_cases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(failed_cases[0]) & set(failed_cases[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"resultados1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failed_cases[0] == failed_cases[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_cases = flatten_list(failed_cases)\n",
    "counter = Counter(chain(failed_cases))\n",
    "counter = sorted(counter.items(), key=lambda x: x[1], reverse = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
