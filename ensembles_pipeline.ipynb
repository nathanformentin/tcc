{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n",
    "from imblearn.datasets import fetch_datasets\n",
    "X, Y = [], []\n",
    "# dataset_names = ['ecoli']\n",
    "dataset_names = ['ecoli', 'optical_digits',\n",
    "                 'satimage', 'pen_digits',\n",
    "                 'abalone', 'sick_euthyroid', 'spectrometer',\n",
    "                 'car_eval_34', 'isolet', 'us_crime', 'yeast_ml8',\n",
    "                 'scene']\n",
    "#dataset_names = ['ecoli']\n",
    "for ds_name in dataset_names:\n",
    "    var = fetch_datasets()[ds_name]\n",
    "    X.append(var['data'])\n",
    "    Y.append(var['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from ipynb.fs.full.data_wrangling import * #Data preprocessing notebook\n",
    "#from ipynb.fs.full.data_preparation import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "SCORINGS = {\n",
    "    'f1': make_scorer(f1_score, average = None),\n",
    "    'precision': make_scorer(precision_score, average = None),\n",
    "    'recall': make_scorer(recall_score, average = None),\n",
    "    'roc_auc': make_scorer(roc_auc_score, average = None)\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Config individual experiment\n",
    "\"\"\"\n",
    "\n",
    "MODELS_TO_OPTIMIZE = [\"K-Nearest Neighbors\", \"Logistic Regression\", \"Support Vector Machines\",\n",
    "                     \"Naive Bayes\", \"Decision Tree\", \"AdaBoost\", \"Random Forest\",  \"Bagging (DT)\",\n",
    "                     \"Extra Trees\"]\n",
    "\n",
    "POSSIBLE_ESTIMATORS_FOR_ADABOOST = [\"Decision Tree\", \"Random Forest\", \"Support Vector Machines\"]\n",
    "\n",
    "cv_splits = 10\n",
    "repetitions = 1\n",
    "RANDOM_STATE = 42\n",
    "cross_validation_setting = RepeatedStratifiedKFold(n_splits=cv_splits,\n",
    "                                                   n_repeats=repetitions,\n",
    "                                                   random_state= RANDOM_STATE)\n",
    "        \n",
    "def cross_validate(model, X_train, X_test, y_train, y_test, metric):\n",
    "    for index in range(len(X_train)):\n",
    "        model.fit(X_train, y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "def model_evaluation(model, features, target, \n",
    "                     cv = cross_validation_setting):\n",
    "    scores = dict()\n",
    "    formatted_scores = dict()\n",
    "    formatted_scores['model'] = model\n",
    "    for scoring_name, scoring_function in SCORINGS.items():\n",
    "        scores[score_metric] = cross_validate(model, X, Y, \n",
    "                                              scoring = scoring_function,\n",
    "                                              cv = cross_validation_setting)\n",
    "        return scores[score_metric]\n",
    "\n",
    "def get_scores(Y_pred, Y_true):\n",
    "    f1 = f1_score(Y_true, Y_pred, average=None)\n",
    "    precision = precision_score(Y_true, Y_pred,\n",
    "                                average = None)\n",
    "    recall = recall_score(Y_true, Y_pred, \n",
    "                          average = None)\n",
    "    roc = roc_auc_score(Y_true, Y_pred, \n",
    "                        average = None)\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    \n",
    "    return f1, precision, recall, roc, acc\n",
    "\n",
    "def convert_df(X, Y):\n",
    "    return X.to_numpy(), Y.to_numpy()\n",
    "\n",
    "def fault_cases(predictions, answers, indexes):\n",
    "    failed_cases = []\n",
    "    for i in range(len(predictions)):\n",
    "        if (predictions[i] != answers[i]):\n",
    "            failed_cases.append(indexes[i])\n",
    "    return failed_cases\n",
    "\n",
    "def get_best_estimator(model, param_grid, X, Y):\n",
    "    model_gridsearch = GridSearchCV(model, param_grid#,\n",
    "                                    #refit = True\n",
    "                                   )\n",
    "    return model_gridsearch.fit(X, Y).best_estimator_\n",
    "    \n",
    "def evaluate_model(dataset_name, models,\n",
    "                   hyperparameters_grid, X, Y, sk_fold,\n",
    "                   metric = \"F1 Average B\", sampling = ''):\n",
    "    \n",
    "    best_metric_score = 0\n",
    "    best_failed_cases = []\n",
    "    estimators = dict()\n",
    "    folds = sk_fold.split(X, Y)\n",
    "    model_names = list(models.keys())\n",
    "    aux_table = pd.DataFrame()\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        if model_name in MODELS_TO_OPTIMIZE:\n",
    "            estimator = get_best_estimator(models[model_name],\n",
    "                                           hyperparameters_grid[model_name],\n",
    "                                           X, Y)\n",
    "        else:\n",
    "            estimator = model[model_name]\n",
    "\n",
    "        f1_list, precision_list, recall_list, auc_list, acc_list = [], [], [], [], []\n",
    "        aux_failed_cases_index, failed_cases_index = [], []\n",
    "        for train_index, test_index in sk_fold.split(X, Y):\n",
    "\n",
    "            model_estimator = estimator\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            model_estimator.fit(X_train, Y_train)\n",
    "            Y_pred = model_estimator.predict(X_test)\n",
    "            \n",
    "            f1, precision, recall, roc, acc = get_scores(Y_pred, Y_test)\n",
    "\n",
    "            failed_cases_index.append(fault_cases(Y_pred, Y_test, test_index))\n",
    "            f1_list.append(f1)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            auc_list.append(roc)\n",
    "            acc_list.append(acc)\n",
    "            \n",
    "        estimators[model_name] = estimator\n",
    "            \n",
    "        aux_table, aux_failed_cases_index =  format_return(dataset_name, model_name,\n",
    "                                                           f1_list, precision_list,\n",
    "                                                           recall_list, auc_list,\n",
    "                                                           acc_list, metric,\n",
    "                                                           sampling), failed_cases_index[0]\n",
    "        if aux_table.iloc[0][metric] > best_metric_score:\n",
    "            best_model_table = aux_table\n",
    "            best_metric_score = aux_table.iloc[0][metric]\n",
    "            best_failed_cases = aux_failed_cases_index\n",
    "            best_estimator = estimators[model_name]\n",
    "            best_estimator_name = model_name\n",
    "            \n",
    "    return estimators, best_model_table, best_failed_cases, best_estimator, best_estimator_name\n",
    "\n",
    "WEAK_LEARNER_COLUMNS = [\"Dataset Name\", \"Base Model Name\", \"F1 Average B\", \"F1 Class 0 B\", \"F1 Class 1 B\",\n",
    "           \"Recall Average B\", \"Recall Class 0 B\", \"Recall Class 1 B\",\n",
    "           \"Precision Average B\", \"Precision Class 0 B\", \"Precision Class 1 B\",\n",
    "           \"AUC B\", \"Accuracy B\"]\n",
    "\n",
    "ENSEMBLE_COLUMNS = [\"Dataset Name\", \"Ensemble Model Name\", \"F1 Average E\", \"F1 Class 0 E\", \"F1 Class 1 E\",\n",
    "                   \"Recall Average E\", \"Recall Class 0 E\", \"Recall Class 1 E\",\n",
    "                   \"Precision Average E\", \"Precision Class 0 E\", \"Precision Class 1 E\",\n",
    "                   \"AUC E\", \"Accuracy E\"]\n",
    "\n",
    "RESULT = [\"Dataset Name\", \"Base Model Name\", \"F1 Average B\", \"F1 Class 0 B\", \"F1 Class 1 B\",\n",
    "           \"Recall Average B\", \"Recall Class 0 B\", \"Recall Class 1 B\",\n",
    "           \"Precision Average B\", \"Precision Class 0 B\", \"Precision Class 1 B\",\n",
    "           \"AUC B\", \"Accuracy B\", \"Ensemble Model Name\", \"F1 Average E\", \"F1 Class 0 E\", \"F1 Class 1 E\",\n",
    "           \"Recall Average E\", \"Recall Class 0 E\", \"Recall Class 1 E\",\n",
    "           \"Precision Average E\", \"Precision Class 0 E\", \"Precision Class 1 E\",\n",
    "           \"AUC E\", \"Accuracy E\"]\n",
    "\n",
    "def format_return(dataset_name, model_name, f1_list, precision_list,\n",
    "                  recall_list, auc_list, acc_list, metric, sampling = ''):\n",
    "    if (metric == WEAK_LEARNER_METRIC):\n",
    "        COLUMNS = WEAK_LEARNER_COLUMNS\n",
    "    else:\n",
    "        COLUMNS = ENSEMBLE_COLUMNS\n",
    "    dataframe = pd.DataFrame(columns = COLUMNS)\n",
    "    dataframe_line = []\n",
    "    dataframe_line.append(dataset_name + sampling)\n",
    "    dataframe_line.append(model_name)\n",
    "    aux = []\n",
    "    lists_of_score_list = []\n",
    "    lists_of_score_list.append(f1_list)\n",
    "    lists_of_score_list.append(precision_list)\n",
    "    lists_of_score_list.append(recall_list)\n",
    "    for score_list in lists_of_score_list:\n",
    "        dataframe_line.extend((statistics.mean(flatten_list(score_list)),\n",
    "                               statistics.mean([score[0] for score in score_list]),\n",
    "                               statistics.mean([score[1] for score in score_list])))\n",
    "\n",
    "    dataframe_line.append(statistics.mean(auc_list))\n",
    "    dataframe_line.append(statistics.mean(acc_list))\n",
    "    return dataframe.append(pd.Series(dataframe_line, index = COLUMNS),\n",
    "                            ignore_index = True)\n",
    "\n",
    "def flatten_list(lista):\n",
    "    return [value for sublist in lista for value in sublist]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Config for every experiment\n",
    "\"\"\"\n",
    "RANDOM_STATE = 0\n",
    "VOTING_METHOD = 'hard'\n",
    "#results = pd.DataFrame(columns = COLUMNS)\n",
    "\n",
    "param_grids = dict()\n",
    "weak_learners_base_models = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Weak Learners\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "KNN\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "k_range = list(range(1,31))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "\n",
    "knn_grid = dict(n_neighbors = k_range, \n",
    "                weights = weight_options)\n",
    "\n",
    "param_grids[\"K-Nearest Neighbors\"] = knn_grid\n",
    "knn = KNeighborsClassifier()\n",
    "weak_learners_base_models[\"K-Nearest Neighbors\"] = knn\n",
    "\n",
    "\"\"\"\n",
    "LR\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_grid={'C':[0.001,0.01,.09,1,5,10],\n",
    "              \"penalty\":[\"l1\",\"l2\"]} #l1 lasso l2 ridge\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "param_grids[\"Logistic Regression\"] = log_reg_grid\n",
    "weak_learners_base_models[\"Logistic Regression\"] = lr\n",
    "\n",
    "\"\"\"\n",
    "Support Vector Machines\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']}\n",
    "\n",
    "param_grids[\"Support Vector Machines\"] = svm_grid\n",
    "weak_learners_base_models[\"Support Vector Machines\"] = SVC()\n",
    "\"\"\"\n",
    "Naive Bayes\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "weak_learners_base_models[\"Naive Bayes\"] = nb\n",
    "param_grids[\"Naive Bayes\"] = nb_params\n",
    "\n",
    "\"\"\"\n",
    "DECISION TREES\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_parameters = {'criterion':['gini','entropy'],\n",
    "             'max_depth':[4,5,10]}\n",
    "\n",
    "param_grids['Decision Tree'] = tree_parameters\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "weak_learners_base_models['Decision Tree'] = dt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensembles(best_base_estimators, best_estimator, best_estimator_name):\n",
    "    \"\"\"\n",
    "    ENSEMBLES\n",
    "    \"\"\"\n",
    "    #AdaBoost\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    ensembles = dict()\n",
    "    ensembles_param_grids = dict()\n",
    "\n",
    "    ab_param_grid = {\n",
    "        'n_estimators' : [400, 600],\n",
    "        'learning_rate' : [1e-3, 1e-2, 1e-1, 1],\n",
    "        'algorithm' : ['SAMME']\n",
    "    }\n",
    "    if best_estimator_name in POSSIBLE_ESTIMATORS_FOR_ADABOOST:\n",
    "        ab_model = AdaBoostClassifier(base_estimator = best_estimator,\n",
    "                                      random_state = RANDOM_STATE)\n",
    "    else:\n",
    "        ab_model = AdaBoostClassifier(base_estimator = best_base_estimators['Support Vector Machines'],\n",
    "                                      random_state = RANDOM_STATE)\n",
    "\n",
    "    ensembles_param_grids['AdaBoost'] = ab_param_grid\n",
    "    ensembles['AdaBoost'] = ab_model\n",
    "\n",
    "\n",
    "    #RandomForest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rf_grid = {\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "#         'max_depth': [4, 5, 6],\n",
    "        'min_samples_leaf': [3, 5, 10],\n",
    "#         'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [200, 400, 600],\n",
    "        'max_features' : ['auto', 'log2']\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "    ensembles_param_grids['Random Forest'] = rf_grid\n",
    "    ensembles['Random Forest'] = rf\n",
    "\n",
    "    #Bagging\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "    bagging_param_grid = {\n",
    "        #'base_estimator__max_depth' : [ 4, 5],\n",
    "        'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "    }\n",
    "\n",
    "    bg_clf = BaggingClassifier(base_estimator=best_estimator,\n",
    "                               random_state = RANDOM_STATE)\n",
    "\n",
    "    ensembles_param_grids['Bagging (DT)'] = bagging_param_grid\n",
    "    ensembles['Bagging (DT)'] = bg_clf\n",
    "\n",
    "    #Extra Trees\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    et_grid = {\n",
    "        'max_depth': [4, 5, 6],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [200, 400],\n",
    "        'oob_score': [True, False]\n",
    "    }\n",
    "\n",
    "    et_clf = ExtraTreesClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "    ensembles_param_grids['Extra Trees'] = et_grid\n",
    "\n",
    "    ensembles['Extra Trees'] = et_clf\n",
    "\n",
    "    #Stacking \n",
    "\n",
    "    \"\"\"Stacking Ensemble\"\"\"\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "    stacking_estimators = [\n",
    "        ('lr', best_base_estimators['Logistic Regression']),\n",
    "        ('knn', best_base_estimators['K-Nearest Neighbors']),\n",
    "        ('svm', best_base_estimators['Support Vector Machines']),\n",
    "        ('gnb', best_base_estimators['Naive Bayes']),\n",
    "        ('dt',  best_base_estimators['Decision Tree'])\n",
    "    ]\n",
    "\n",
    "    final_estimator = best_estimator\n",
    "\n",
    "    stacking_model = StackingClassifier(estimators = stacking_estimators,\n",
    "                                        final_estimator = final_estimator)\n",
    "\n",
    "    \"\"\"Voting Ensemble\"\"\"\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "    voting_estimators = [\n",
    "        ('lr', best_base_estimators['Logistic Regression']),\n",
    "        ('knn', best_base_estimators['K-Nearest Neighbors']),\n",
    "        ('svm', best_base_estimators['Support Vector Machines']),\n",
    "        ('gnb', best_base_estimators['Naive Bayes']),\n",
    "        ('dt', best_base_estimators['Decision Tree'])\n",
    "    ]\n",
    "\n",
    "    VOTING_METHOD = 'hard'\n",
    "\n",
    "    voting_classifier = VotingClassifier(voting_estimators,\n",
    "                                         voting=VOTING_METHOD)\n",
    "    \n",
    "    return ensembles, ensembles_param_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns = WEAK_LEARNER_COLUMNS)\n",
    "dataframe2 = pd.DataFrame(columns = ENSEMBLE_COLUMNS)\n",
    "result = pd.DataFrame(columns = RESULT)\n",
    "cases = []\n",
    "WEAK_LEARNER_METRIC = \"F1 Average B\"\n",
    "ENSEMBLE_LEARNER_METRIC = \"F1 Average E\"\n",
    "best_base_estimators = dict()\n",
    "best_base_clfs = []\n",
    "for index in range(len(dataset_names)):\n",
    "    \n",
    "    data = X[index]\n",
    "    target = Y[index]\n",
    "    \n",
    "    best_estimators, model_scores, _, current_best_clf, best_estimator_name = evaluate_model(\n",
    "                   dataset_names[index], weak_learners_base_models, param_grids, data, target,\n",
    "                   cross_validation_setting, metric = WEAK_LEARNER_METRIC, sampling = '')\n",
    "    \n",
    "    print(current_best_clf)\n",
    "    \n",
    "    ensembles, ensembles_param_grids = generate_ensembles(best_estimators,\n",
    "                                                          current_best_clf,\n",
    "                                                          best_estimator_name)\n",
    "    _, ensemble_scores, ensemble_error_cases, _, _ = evaluate_model(dataset_names[index], \n",
    "                   ensembles, ensembles_param_grids, data, target,\n",
    "                   cross_validation_setting, metric = ENSEMBLE_LEARNER_METRIC)\n",
    "    print(ensemble_scores)\n",
    "    dataframe = dataframe.append(model_scores)\n",
    "    dataframe2 = dataframe2.append(ensemble_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#UNDERSAMPLING\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "for index in range(len(dataset_names)):\n",
    "    data = X[index]\n",
    "    target = Y[index]\n",
    "    X_under, y_under = undersample.fit_resample(data, target)\n",
    "    best_estimators, model_scores, _, current_best_clf, best_estimator_name = evaluate_model(\n",
    "                   dataset_names[index], weak_learners_base_models, param_grids, X_under, y_under,\n",
    "                   cross_validation_setting, metric = WEAK_LEARNER_METRIC, sampling = ' Undersampled')\n",
    "    \n",
    "    print(current_best_clf)\n",
    "    \n",
    "    ensembles, ensembles_param_grids = generate_ensembles(best_estimators,\n",
    "                                                          current_best_clf,\n",
    "                                                          best_estimator_name)\n",
    "    _, ensemble_scores, ensemble_error_cases, _, _ = evaluate_model(dataset_names[index], \n",
    "                   ensembles, ensembles_param_grids, X_under, y_under,\n",
    "                   cross_validation_setting, metric = ENSEMBLE_LEARNER_METRIC)\n",
    "    print(ensemble_scores)\n",
    "    dataframe = dataframe.append(model_scores)\n",
    "    dataframe2 = dataframe2.append(ensemble_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#OVERSAMPLING\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler()\n",
    "\n",
    "for index in range(len(dataset_names)):\n",
    "    \n",
    "    data = X[index]\n",
    "    target = Y[index]\n",
    "    X_over, y_over = oversample.fit_resample(data, target)\n",
    "    \n",
    "    best_estimators, model_scores, _, current_best_clf, best_estimator_name = evaluate_model(\n",
    "                   dataset_names[index], weak_learners_base_models, param_grids, X_over, y_over,\n",
    "                   cross_validation_setting, metric = WEAK_LEARNER_METRIC, sampling = ' Oversampled')\n",
    "    \n",
    "    \n",
    "    ensembles, ensembles_param_grids = generate_ensembles(best_estimators,\n",
    "                                                          current_best_clf,\n",
    "                                                          best_estimator_name)\n",
    "    _, ensemble_scores, ensemble_error_cases, _, _ = evaluate_model(dataset_names[index], \n",
    "                   ensembles, ensembles_param_grids, X_over, y_over,\n",
    "                   cross_validation_setting, metric = ENSEMBLE_LEARNER_METRIC)\n",
    "    dataframe = dataframe.append(model_scores)\n",
    "    dataframe2 = dataframe2.append(ensemble_scores)\n",
    "    \n",
    "aux_df = dataframe.reset_index()\n",
    "dataframe2 = dataframe2.reset_index()\n",
    "aux_df = aux_df.join(dataframe2, lsuffix='', rsuffix='_to_delete')\n",
    "result = aux_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ENSEMBLES\n",
    "\"\"\"\n",
    "\n",
    "#AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ensembles = dict()\n",
    "ensembles_param_grids = dict()\n",
    "\n",
    "ab_param_grid = {\n",
    "    'n_estimators' : [100, 300, 500],\n",
    "    'learning_rate' : [1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "ab_model = AdaBoostClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "ensembles_param_grids['AdaBoost'] = ab_param_grid\n",
    "ensembles['AdaBoost'] = ab_model\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [200, 400]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "ensembles_param_grids['Random Forest'] = rf_grid\n",
    "ensembles['Random Forest'] = rf\n",
    "\n",
    "#Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_param_grid = {\n",
    "    'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "bg_clf = BaggingClassifier(base_estimator=best_base_estimators['Decision Tree'],\n",
    "                           random_state = RANDOM_STATE)\n",
    "\n",
    "ensembles_param_grids['Bagging (DT)'] = bagging_param_grid\n",
    "ensembles['Bagging (DT)'] = bg_clf\n",
    "\n",
    "#Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [200, 400],\n",
    "    'oob_score': [True, False]\n",
    "}\n",
    "\n",
    "et_clf = ExtraTreesClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "ensembles_param_grids['Extra Trees'] = et_grid\n",
    "\n",
    "ensembles['Extra Trees'] = et_clf\n",
    "\n",
    "#Stacking \n",
    "\n",
    "\"\"\"Stacking Ensemble\"\"\"\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('lr', best_base_estimators['Logistic Regression']),\n",
    "    ('knn', best_base_estimators['K-Nearest Neighbors']),\n",
    "    ('svm', best_base_estimators['Support Vector Machines']),\n",
    "    ('gnb', best_base_estimators['Naive Bayes']),\n",
    "    ('dt',  best_base_estimators['Decision Tree'])\n",
    "]\n",
    "\n",
    "final_estimator = best_base_estimators['Logistic Regression']\n",
    "\n",
    "stacking_model = StackingClassifier(estimators = stacking_estimators,\n",
    "                                    final_estimator = final_estimator)\n",
    "\n",
    "\"\"\"Voting Ensemble\"\"\"\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_estimators = [\n",
    "    ('lr', best_base_estimators['Logistic Regression']),\n",
    "    ('knn', best_base_estimators['K-Nearest Neighbors']),\n",
    "    ('svm', best_base_estimators['Support Vector Machines']),\n",
    "    ('gnb', best_base_estimators['Naive Bayes']),\n",
    "    ('dt', best_base_estimators['Decision Tree'])\n",
    "]\n",
    "\n",
    "VOTING_METHOD = 'hard'\n",
    "\n",
    "voting_classifier = VotingClassifier(voting_estimators,\n",
    "                                     voting=VOTING_METHOD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
